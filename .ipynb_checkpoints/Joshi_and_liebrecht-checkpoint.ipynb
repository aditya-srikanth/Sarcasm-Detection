{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk import TreebankWordTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset and preprocess\n",
    "data = pd.read_csv('sarcasm_dataset_clean.csv')          # import the data\n",
    "labels = data['label'].tolist()                          # extracts the labels as a list \n",
    "text = data['text'].tolist()                             # extracts the text to be processed as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok = TreebankWordTokenizer()\n",
    "# tokenizer=tok.tokenize, use this if the results are poor\n",
    "minfreq = 3                                              # Used to prune those terms (uni-|bi-|tri- grams whose frequency is less than this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you remember me telling you we are practicing non-verbal spells, Potter?\" \"Yes,\" said Harry stiffly. \"Yes, sir.\" \"There's no need to call me \"sir\" Professor.\" The words had escaped him before he knew what he was saying.  '\n"
     ]
    }
   ],
   "source": [
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(text,labels,n_gram,tokenizer=None,min_document_frequency=2,minfreq=3):\n",
    "    '''\n",
    "        Tokenizes the dataset and return the term-frequency matrix of the frequent n-grams \n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        text : list\n",
    "            List of strings containing the text to be processed.\n",
    "        n_gram: int\n",
    "            The size of the n-gram to be returned\n",
    "        tokenizer:\n",
    "            String tokenizer to be used\n",
    "        min_document_frequency: \n",
    "            The minimum document frequency to be used to prune n-grams\n",
    "        minfreq: \n",
    "            The minimum frequency to be used to prune n-grams\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dataframe : Pandas DataFrame\n",
    "            The text converted as a term-frequency matrix\n",
    "        n_grams : list\n",
    "            The list of n-grams that were generated\n",
    "        weights: tuple \n",
    "            p_value , chi2 value \n",
    "    '''\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenizer,lowercase=False,\\\n",
    "                                 ngram_range=(n_gram,n_gram),\\\n",
    "                                 min_df=min_document_frequency,\\\n",
    "                                 stop_words='english',\\\n",
    "                                 token_pattern='\\\\w+')                     \n",
    "    processed_data = (vectorizer.fit_transform(text))                      \n",
    "    processed_data = processed_data.toarray()                              \n",
    "    \n",
    "    n_grams = vectorizer.get_feature_names()                               \n",
    "    \n",
    "    counts = np.sum(processed_data,axis=0)                                 \n",
    "    indices_to_keep = (np.argwhere(counts > minfreq)).flatten()            \n",
    "    processed_data = processed_data[:,indices_to_keep]                       \n",
    "    \n",
    "    dataframe = pd.DataFrame(processed_data)\n",
    "    \n",
    "    weights = chi2(processed_data,labels)\n",
    "    \n",
    "    return dataframe,n_grams,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_matrix,unigrams,(unigram_p,unigram_chi) = process_data(text,labels,1)\n",
    "unigram_features = unigrams_matrix*unigram_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_matrix,bigrams,(bigram_p,bigram_chi) = process_data(text,labels,2)\n",
    "bigram_features = bigrams_matrix*bigram_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_matrix,trigrams,(trigram_p,trigram_chi) = process_data(text,labels,3)\n",
    "trigram_features = trigrams_matrix*trigram_chi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
