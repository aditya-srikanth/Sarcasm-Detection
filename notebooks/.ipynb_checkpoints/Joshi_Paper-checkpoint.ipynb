{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPh1BVBwfTLV",
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishnu\\Miniconda3\\envs\\tf\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import *\n",
    "import gensim\n",
    "import collections\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import shlex\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from weakref import proxy\n",
    "from pprint import pprint\n",
    "# Does not display warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "BKEssi6Ew5VB"
   },
   "source": [
    "### Download Required Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "vhwIeLVIwwtw"
   },
   "source": [
    "#### Download all NLTK packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "NeHNpksthgnA"
   },
   "outputs": [],
   "source": [
    "!python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ykiFe_LeyNW3"
   },
   "source": [
    "#### Download External Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "hidden": true,
    "id": "IhR_PE3BhZa0",
    "outputId": "307c9296-5bb6-4404-f93b-251581f41f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-25 08:47:47--  https://docs.google.com/uc?id=0B3X9GlR6EmbnQ0FtZmJJUXEyRTA\n",
      "Resolving docs.google.com (docs.google.com)... 74.125.203.100, 74.125.203.102, 74.125.203.138, ...\n",
      "Connecting to docs.google.com (docs.google.com)|74.125.203.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-08-48-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9r6j7n8pbmg5b991p4lgqmr38b6qjqiu/1551081600000/15876260727594163214/*/0B3X9GlR6EmbnQ0FtZmJJUXEyRTA [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2019-02-25 08:47:48--  https://doc-08-48-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/9r6j7n8pbmg5b991p4lgqmr38b6qjqiu/1551081600000/15876260727594163214/*/0B3X9GlR6EmbnQ0FtZmJJUXEyRTA\n",
      "Resolving doc-08-48-docs.googleusercontent.com (doc-08-48-docs.googleusercontent.com)... 64.233.189.132, 2404:6800:4008:c07::84\n",
      "Connecting to doc-08-48-docs.googleusercontent.com (doc-08-48-docs.googleusercontent.com)|64.233.189.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-executable]\n",
      "Saving to: ‘uc?id=0B3X9GlR6EmbnQ0FtZmJJUXEyRTA’\n",
      "\n",
      "\r",
      "          uc?id=0B3     [<=>                 ]       0  --.-KB/s               \r",
      "uc?id=0B3X9GlR6Embn     [ <=>                ]   7.44M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2019-02-25 08:47:48 (75.3 MB/s) - ‘uc?id=0B3X9GlR6EmbnQ0FtZmJJUXEyRTA’ saved [7805504]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download gdrive bash command\n",
    "!wget -c https://docs.google.com/uc?id=0B3X9GlR6EmbnQ0FtZmJJUXEyRTA&export=download\n",
    "!cp 'uc?id=0B3X9GlR6EmbnQ0FtZmJJUXEyRTA' /usr/local/bin/gdrive\n",
    "!chmod a+x /usr/local/bin/gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "hidden": true,
    "id": "dRkloskQw9nE",
    "outputId": "fabe2b50-6712-4173-9812-ebeca8ccb5f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication needed\n",
      "Go to the following url in your browser:\n",
      "https://accounts.google.com/o/oauth2/auth?access_type=offline&client_id=367116221053-7n0vf5akeru7on6o2fjinrecpdoe99eg.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=state\n",
      "\n",
      "Enter verification code: 4/_AD-CLg3bql9pkZIwaSTW4GMu7LvhK1lK2vb9wLUibvILn-Xf3FRSuI\n",
      "Downloading GoogleNews-vectors-negative300.bin.gz -> GoogleNews-vectors-negative300.bin.gz\n",
      "Downloaded 0B7XkCwpI5KDYNlNUTTlSS21pQmM at 135.3 MB/s, total 1.6 GB\n"
     ]
    }
   ],
   "source": [
    "# Download Word2Vec Vectors and Verify using any gmail account\n",
    "!gdrive download 0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
    "!gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "hidden": true,
    "id": "a2pe8f6CxHS6",
    "outputId": "394ece6b-3ee9-4fb8-de4d-1fb901fc5c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-02-25 08:49:19--  http://download.joachims.org/svm_perf/current/svm_perf_linux64.tar.gz\n",
      "Resolving download.joachims.org (download.joachims.org)... 81.88.42.187, 81.88.34.174\n",
      "Connecting to download.joachims.org (download.joachims.org)|81.88.42.187|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: http://osmot.cs.cornell.edu/svm_perf/current/svm_perf_linux64.tar.gz [following]\n",
      "--2019-02-25 08:49:20--  http://osmot.cs.cornell.edu/svm_perf/current/svm_perf_linux64.tar.gz\n",
      "Resolving osmot.cs.cornell.edu (osmot.cs.cornell.edu)... 128.253.51.182\n",
      "Connecting to osmot.cs.cornell.edu (osmot.cs.cornell.edu)|128.253.51.182|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 150144 (147K) [application/x-gzip]\n",
      "Saving to: ‘svm_perf_linux64.tar.gz’\n",
      "\n",
      "svm_perf_linux64.ta 100%[===================>] 146.62K   257KB/s    in 0.6s    \n",
      "\n",
      "2019-02-25 08:49:21 (257 KB/s) - ‘svm_perf_linux64.tar.gz’ saved [150144/150144]\n",
      "\n",
      "LICENSE.txt\n",
      "svm_perf_learn\n",
      "svm_perf_classify\n"
     ]
    }
   ],
   "source": [
    "# Download SVMperf\n",
    "!wget http://download.joachims.org/svm_perf/current/svm_perf_linux64.tar.gz\n",
    "!gunzip svm_perf_linux64.tar.gz\n",
    "!tar -xvf svm_perf_linux64.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7dCSzqbfTLo"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2w-0-M5AfTLs"
   },
   "outputs": [],
   "source": [
    "# Upload Dataset\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkVBIl7PfeLH"
   },
   "outputs": [],
   "source": [
    "# Load Dataset to pandas Dataframe\n",
    "original_data = pd.read_csv('../joshi_data/train', delimiter='\\t', names=['text', 'label'])\n",
    "labels = np.array(original_data['label'])\n",
    "labels = pd.DataFrame(labels).replace('sarcasm', 1)\n",
    "labels = labels.replace('philosophy', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "test_data = pd.read_csv('../joshi_data/test', delimiter='\\t', names=['text', 'label'])\n",
    "labels_test = np.array(test_data['label'])\n",
    "labels_test = pd.DataFrame(labels_test).replace('sarcasm', 1)\n",
    "labels_test = labels_test.replace('philosophy', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXwiitn-fTL3"
   },
   "outputs": [],
   "source": [
    "# Shuffle if required\n",
    "data_shuffle = original_data.sample(frac=1).reset_index(drop=True)\n",
    "labels = np.array(data_shuffle['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEokp421fTMB"
   },
   "outputs": [],
   "source": [
    "# Load Pretrained Vectors\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40BYKqh7i3RE"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "guxy3ByVfTMM"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "EAJ38tzEfTMQ"
   },
   "outputs": [],
   "source": [
    "def process_data(text,labels,n_gram,tokenizer=None,min_document_frequency=2,minfreq=3):\n",
    "    '''\n",
    "        Tokenizes the dataset and return the term-frequency matrix of the frequent n-grams \n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        text : list\n",
    "            List of strings containing the text to be processed.\n",
    "        n_gram: int\n",
    "            The size of the n-gram to be returned\n",
    "        tokenizer:\n",
    "            String tokenizer to be used\n",
    "        min_document_frequency: \n",
    "            The minimum document frequency to be used to prune n-grams\n",
    "        minfreq: \n",
    "            The minimum frequency to be used to prune n-grams\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dataframe : Pandas DataFrame\n",
    "            The text converted as a term-frequency matrix\n",
    "        n_grams : list\n",
    "            The list of n-grams that were generated\n",
    "        weights: tuple \n",
    "            p_value , chi2 value \n",
    "    '''\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenizer,lowercase=False,\\\n",
    "                                 ngram_range=(n_gram,n_gram),\\\n",
    "                                 min_df=min_document_frequency,\\\n",
    "                                 stop_words='english',\\\n",
    "                                 token_pattern='\\\\w+')\n",
    "    \n",
    "    processed_data = (vectorizer.fit_transform(text))                      \n",
    "    processed_data = processed_data.toarray()                              \n",
    "    \n",
    "    n_grams = vectorizer.get_feature_names()                               \n",
    "    \n",
    "    counts = np.sum(processed_data,axis=0)                                 \n",
    "    indices_to_keep = (np.argwhere(counts > minfreq)).flatten()            \n",
    "    processed_data = processed_data[:,indices_to_keep] \n",
    "    n_grams = [n_grams[i]  for i in indices_to_keep]\n",
    "    dataframe = pd.DataFrame(processed_data)\n",
    "    \n",
    "    weights = chi2(processed_data,labels)\n",
    "    \n",
    "    return dataframe,n_grams,weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJ7EEm7MfTMZ"
   },
   "source": [
    "### Word Embedding based features (Joshi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJNvW5LQfTMb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Link(object):\n",
    "    __slots__ = 'prev', 'next', 'key', '__weakref__'\n",
    "\n",
    "class OrderedSet(collections.MutableSet):\n",
    "    'Set the remembers the order elements were added'\n",
    "    # Big-O running times for all methods are the same as for regular sets.\n",
    "    # The internal self.__map dictionary maps keys to links in a doubly linked list.\n",
    "    # The circular doubly linked list starts and ends with a sentinel element.\n",
    "    # The sentinel element never gets deleted (this simplifies the algorithm).\n",
    "    # The prev/next links are weakref proxies (to prevent circular references).\n",
    "    # Individual links are kept alive by the hard reference in self.__map.\n",
    "    # Those hard references disappear when a key is deleted from an OrderedSet.\n",
    "\n",
    "    def __init__(self, iterable=None):\n",
    "        self.__root = root = Link()         # sentinel node for doubly linked list\n",
    "        root.prev = root.next = root\n",
    "        self.__map = {}                     # key --> link\n",
    "        if iterable is not None:\n",
    "            self |= iterable\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__map)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.__map\n",
    "\n",
    "    def add(self, key):\n",
    "        # Store new key in a new link at the end of the linked list\n",
    "        if key not in self.__map:\n",
    "            self.__map[key] = link = Link()            \n",
    "            root = self.__root\n",
    "            last = root.prev\n",
    "            link.prev, link.next, link.key = last, root, key\n",
    "            last.next = root.prev = proxy(link)\n",
    "\n",
    "    def discard(self, key):\n",
    "        # Remove an existing item using self.__map to find the link which is\n",
    "        # then removed by updating the links in the predecessor and successors.        \n",
    "        if key in self.__map:        \n",
    "            link = self.__map.pop(key)\n",
    "            link.prev.next = link.next\n",
    "            link.next.prev = link.prev\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Traverse the linked list in order.\n",
    "        root = self.__root\n",
    "        curr = root.next\n",
    "        while curr is not root:\n",
    "            yield curr.key\n",
    "            curr = curr.next\n",
    "\n",
    "    def __reversed__(self):\n",
    "        # Traverse the linked list in reverse order.\n",
    "        root = self.__root\n",
    "        curr = root.prev\n",
    "        while curr is not root:\n",
    "            yield curr.key\n",
    "            curr = curr.prev\n",
    "\n",
    "    def pop(self, last=True):\n",
    "        if not self:\n",
    "            raise KeyError('set is empty')\n",
    "        key = next(reversed(self)) if last else next(iter(self))\n",
    "        self.discard(key)\n",
    "        return key\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self:\n",
    "            return '%s()' % (self.__class__.__name__,)\n",
    "        return '%s(%r)' % (self.__class__.__name__, list(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, OrderedSet):\n",
    "            return len(self) == len(other) and list(self) == list(other)\n",
    "        return not self.isdisjoint(other)\n",
    "      \n",
    "# Normalization of input specific to extract joshi features\n",
    "def joshi_normalize(df, lowercase=True ):\n",
    "    # Tokenize the sentences\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    # Stop words list\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    df = df.copy()\n",
    "    # Iterate through pandas dataframe\n",
    "    for i, row in df.iterrows():\n",
    "        # row[1] contains the sentence string type = string\n",
    "        new_str = re.sub(r'\\\\w+', '', row[1], re.I|re.A)\n",
    "        # lowercase (doesnt matter as we are extracting word embedding features only)\n",
    "        if(lowercase):\n",
    "            new_str = new_str.lower()\n",
    "        new_str = new_str.strip()\n",
    "        # tokenize string\n",
    "        tokens = wpt.tokenize(new_str)\n",
    "        # filter stop words\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        # join the tokens back as single string\n",
    "        new_str = ' '.join(filtered_tokens)\n",
    "        # update dataframe\n",
    "        df.iat[i, 1] = new_str\n",
    "    \n",
    "    # Return dataframe with normalized text\n",
    "    return df\n",
    "\n",
    "# Calculate 8 Word Embedding Features both unweighted [0-3] and weighted[4-7]\n",
    "# Param : 'df' should have text as 2nd column; 'model': gensim model for selected pretrained vectors\n",
    "def joshi_features(df, model):\n",
    "    # Tokenizer to tokenize input string\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    # final 8 features for every sentence stored as row\n",
    "    features = []\n",
    "    \n",
    "    # As we modify df, we will make a copy of the object\n",
    "    df = df.copy()\n",
    "    for ind, row in df.iterrows():\n",
    "        # extract string\n",
    "        string = row[1]\n",
    "        # tokenize\n",
    "        tokens = wpt.tokenize(string)\n",
    "        \n",
    "        # Create sim_matrix \n",
    "        length = len(tokens)\n",
    "        \n",
    "        # Unweighted similarity matrix\n",
    "        sim_matrix = np.zeros((length, length), dtype='float32')\n",
    "        sim_matrix.fill(np.nan)\n",
    "        \n",
    "        # Weighted Sim Matrix\n",
    "        wsim_matrix = np.zeros((length, length), dtype='float32')\n",
    "        wsim_matrix.fill(np.nan)\n",
    "        \n",
    "        # Build Matrix\n",
    "        for i in range(length):\n",
    "            for j in range(i+1, length):\n",
    "                try:\n",
    "                    sim_matrix[i][j] = model.similarity(tokens[i], tokens[j])\n",
    "                    if sim_matrix[i][j]==1 :\n",
    "                        # To remove repeated words \n",
    "                        raise Exception\n",
    "                    sim_matrix[j][i] = sim_matrix[i][j]\n",
    "                    \n",
    "                    # Finding semantical distance {POINT TO IMPROVE}\n",
    "                    wsim_matrix[i][j] = np.divide(sim_matrix[i][j], np.square(i-j))\n",
    "                    wsim_matrix[j][i] = wsim_matrix[i][j]\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # We fill nan if the word does not exist in model and ignore\n",
    "                    sim_matrix[i][j] = np.nan\n",
    "                    sim_matrix[j][i] = np.nan\n",
    "                    wsim_matrix[i][j] = np.nan\n",
    "                    wsim_matrix[j][i] = np.nan\n",
    "        \n",
    "        # Create dataframe for easier calulations\n",
    "        sim_df = pd.DataFrame(sim_matrix)\n",
    "        # Find most similar score for every word\n",
    "        sim  = sim_df.max(axis=1)\n",
    "        # Find most dissimilar score for every word\n",
    "        dsim = sim_df.min(axis=1)\n",
    "        \n",
    "        # Create dataframe for easier calculations\n",
    "        wsim_df = pd.DataFrame(wsim_matrix)\n",
    "        # Find most similar score for every word\n",
    "        wsim  = wsim_df.max(axis=1)\n",
    "        # Find most dissimilar score for every word\n",
    "        wdsim = wsim_df.min(axis=1)\n",
    "        \n",
    "        # Extract 8 features\n",
    "        ff = [sim.max(axis=0), sim.min(axis=0), dsim.max(axis=0), dsim.min(axis=0), \n",
    "              wsim.max(axis=0), wsim.min(axis=0), wdsim.max(axis=0), wdsim.min(axis=0)]\n",
    "        features.append(ff)\n",
    "        \n",
    "        # Print Progress\n",
    "        print(ind)\n",
    "    \n",
    "    # Converting list to numpy array\n",
    "    feat = np.array(features)\n",
    "    # Converting numpy array to dataframe with appropriate column names\n",
    "    feat_df = pd.DataFrame(feat, columns=['max_sim', 'min_sim', 'max_dsim', 'min_dsim', \n",
    "                                          'max_wsim', 'min_wsim', 'max_wdsim', 'min_wdsim' ])\n",
    "    # Testing if nan values exist in final feature matrix\n",
    "#      feat_df.isnull().any(1).nonzero()[0]\n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def joshi_features_new(df, model):\n",
    "    # final 8 features for every sentence stored as row\n",
    "    s_features = []\n",
    "    ws_features = []\n",
    "    sws_features = []\n",
    "    \n",
    "    # As we modify df, we will make a copy of the object\n",
    "    df = df.copy()\n",
    "    for ind, row in df.iterrows():\n",
    "        # extract string\n",
    "        string = row['text']\n",
    "        # tokenize\n",
    "        tokens = tokenizer.tokenize(string)\n",
    "        tokens = OrderedSet(tokens)\n",
    "        tokens = list(tokens)\n",
    "        tokens = [x.lower() for x in tokens]\n",
    "    \n",
    "        X = []\n",
    "        accepted_tokens = []\n",
    "        for tok in tokens:\n",
    "            try:\n",
    "                if tok in accepted_tokens:\n",
    "                    continue\n",
    "                X.append(model.word_vec(tok))\n",
    "                accepted_tokens.append(tok)\n",
    "            except Exception as e:\n",
    "                pass   \n",
    "#         print(X)\n",
    "#         print(accepted_tokens)\n",
    "        sim_matrix = cosine_similarity(X)\n",
    "#         print(sim_matrix[0,1])\n",
    "#         print(model.similarity(accepted_tokens[0], accepted_tokens[1]))       \n",
    "#         sim_matrix[sim_matrix>0.999] = np.nan        \n",
    "        pos = []\n",
    "        for tok in accepted_tokens:\n",
    "            pos.append([tokens.index(tok)])\n",
    "        weights = euclidean_distances(pos, squared=True)\n",
    "        wsim_df = pd.DataFrame(sim_matrix/weights).replace(np.Inf, np.nan)\n",
    "        \n",
    "        sim_df = pd.DataFrame(sim_matrix)\n",
    "        sim = sim_df.max(axis=1)\n",
    "        dsim = sim_df.min(axis=1)\n",
    "        # Find most similar score for every word\n",
    "        wsim  = wsim_df.max(axis=1)\n",
    "        # Find most dissimilar score for every word\n",
    "        wdsim = wsim_df.min(axis=1)\n",
    "        \n",
    "        # Extract 8 features\n",
    "        ff = [sim.max(axis=0), sim.min(axis=0), dsim.max(axis=0), dsim.min(axis=0), \n",
    "              wsim.max(axis=0), wsim.min(axis=0), wdsim.max(axis=0), wdsim.min(axis=0)]\n",
    "        sws_features.append(ff)\n",
    "        # Print Progress\n",
    "        print(ind)\n",
    "    \n",
    "    # Testing if nan values exist in final feature matrix\n",
    "    \n",
    "    # Converting numpy array to dataframe with appropriate column names\n",
    "    feat_df = pd.DataFrame(sws_features, columns=['max_sim', 'min_sim', 'max_dsim', 'min_dsim', \n",
    "                                          'max_wsim', 'min_wsim', 'max_wdsim', 'min_wdsim' ])\n",
    "    if(len(feat_df.isnull().any(1).nonzero()[0])==0):\n",
    "        print(\"No Nans\")\n",
    "    else:\n",
    "        feat_df = feat_df.fillna(0)\n",
    "    \n",
    "    s_features  = feat_df.iloc[:, 0:4]\n",
    "    ws_features = feat_df.iloc[:, 4:8]\n",
    "\n",
    "    return feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "kmX2DmCAs5MB"
   },
   "source": [
    "### Liebrecht Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "dl88EL-otE-f"
   },
   "outputs": [],
   "source": [
    "### Add your stuff here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "XtJzOjSAfTMh"
   },
   "source": [
    "### Gonzalez Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "J04TvcEd5tC0"
   },
   "source": [
    "** liwc_features(): **\n",
    "\n",
    "Extracts 32 feature vector(both incidence and frequence) for each sentence using liwc dictionary*\n",
    "\n",
    "*  *df needs to be cleaned text*\n",
    "*  *Very basic implementation can be improved later, for now its slow*\n",
    "*   *Portland stemmer to extract stems of words to match with LIWC dictionary*\n",
    "\n",
    "Returns 2 dataframe (Frequency, Incidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "yozfm-kg4pHc"
   },
   "outputs": [],
   "source": [
    "def liwc_features(df, liwc):\n",
    "    stemmer = PorterStemmer()\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    liwc_f = []\n",
    "    liwc_i = []\n",
    "    for ind, row in df.iterrows():\n",
    "        string = row[1]\n",
    "        tokens = wpt.tokenize(string)\n",
    "        feat = []\n",
    "\n",
    "        for tok in tokens:\n",
    "            tok_stem = stemmer.stem(tok)\n",
    "            tok_stem += '*'\n",
    "            if(tok=='wrote'):\n",
    "                # Ignore this token\n",
    "                continue\n",
    "            if (liwc.index == tok).any():\n",
    "                temp = liwc.loc[tok]\n",
    "                temp = list(temp)\n",
    "                feat.append(temp)\n",
    "            elif (liwc.index == tok_stem).any():\n",
    "                temp = liwc.loc[tok_stem]\n",
    "                temp = list(temp)\n",
    "                feat.append(temp)\n",
    "        if(len(feat)==0):\n",
    "            for i in range(0,len(liwc.columns.values)):\n",
    "                feat.append(0)\n",
    "            freq = feat\n",
    "            inci = feat\n",
    "            liwc_f.append(freq)\n",
    "            liwc_i.append(inci)\n",
    "            print(ind)\n",
    "            continue\n",
    "        temp_df = pd.DataFrame(feat)\n",
    "        freq = temp_df.sum(axis=0).to_list()\n",
    "        inci = []\n",
    "        for i in freq:\n",
    "            if i>=1:\n",
    "                inci.append(1)\n",
    "            else:\n",
    "                inci.append(0)\n",
    "        liwc_f.append(freq)\n",
    "        liwc_i.append(inci)\n",
    "        print(ind)\n",
    "    feature_names = list(liwc.columns.values)\n",
    "    liwcf_df = pd.DataFrame(liwc_f, columns=feature_names)\n",
    "    liwci_df = pd.DataFrame(liwc_i, columns=feature_names)\n",
    "    \n",
    "    return liwcf_df, liwci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Xm_UpOQI8pQB"
   },
   "outputs": [],
   "source": [
    "  def get_gonz_features(df,labels, liwc, n_features=0):\n",
    "    # Load Unigrams\n",
    "    bag_of_words, unigrams, _ = process_data(df['text'], labels, n_gram=1)\n",
    "\n",
    "    # Get the frequency and incidence features from LIWC\n",
    "    liwcf_raw, liwci_raw = liwc_features(df, liwc)\n",
    "\n",
    "    # Check for NAN values\n",
    "    if(len(liwcf_raw.isnull().any(1).to_numpy().nonzero()[0])==0):\n",
    "      print('No Problems')\n",
    "    if(len(liwci_raw.isnull().any(1).to_numpy().nonzero()[0])==0):\n",
    "      print('No Problems')\n",
    "\n",
    "    # Save the feature dataframes\n",
    "    liwcf_raw.to_pickle('liwc_f.pkl')\n",
    "    liwci_raw.to_pickle('liwc_i.pkl')\n",
    "\n",
    "    gonz_liwc_f = liwcf_raw\n",
    "    gonz_liwc_i = liwci_raw\n",
    "\n",
    "    ## chi2 test\n",
    "\n",
    "    weightsf = chi2(liwcf_raw, labels)\n",
    "    weightsi = chi2(liwci_raw, labels)\n",
    "    # create dataframe for frequency\n",
    "    chi2_fdf = pd.DataFrame(weightsf, columns=liwc.columns.values, index=['chi2', 'pval'])\n",
    "    # create dataframe for incidence\n",
    "    chi2_idf = pd.DataFrame(weightsi, columns=liwc.columns.values, index=['chi2', 'pval'])\n",
    "\n",
    "    # 10 Important features\n",
    "    if n_features != 0:\n",
    "      gonz_liwc_f = list(chi2_fdf.loc['chi2'].nlargest(n_features).index)\n",
    "      gonz_liwc_i = list(chi2_idf.loc['chi2'].nlargest(n_features).index)\n",
    "      gonz_liwc_f = liwcf_raw[gonz_liwc_f]\n",
    "      gonz_liwc_i = liwci_raw[gonz_liwc_i]\n",
    "\n",
    "    # Change Names\n",
    "    old_names = list(gonz_liwc_f.columns)\n",
    "    new_names = []\n",
    "    for old in old_names:\n",
    "        new_names.append(old+'{Frequency}')\n",
    "    gonz_liwc_f.columns = new_names\n",
    "\n",
    "    # Change Names\n",
    "    old_names = list(gonz_liwc_i.columns)\n",
    "    new_names = []\n",
    "    for old in old_names:\n",
    "        new_names.append(old+'{Incidence}')\n",
    "    gonz_liwc_i.columns = new_names\n",
    "\n",
    "    bag_of_words = pd.DataFrame(bag_of_words, columns=unigrams )\n",
    "    gonz_features = pd.concat([bag_of_words, gonz_liwc_i, gonz_liwc_f], axis=1)\n",
    "\n",
    "    return gonz_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "AHl3uGhtfTMj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "AukF--A2ynlf"
   },
   "outputs": [],
   "source": [
    "# Load LIWC dictionary (32 feature columns + 1 word name) (4487 words)\n",
    "liwc = pd.read_pickle('liwc32.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBFC5Ca-0-0b"
   },
   "source": [
    "### Create Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R95luT0-1QVj"
   },
   "outputs": [],
   "source": [
    "def getFsets(base_df, joshi_df):\n",
    "  fset1 = base_df\n",
    "  fset2 = joshi_df.iloc[:, 0:4]\n",
    "  fset2 = pd.concat([base_df, fset2], axis=1)\n",
    "  fset3 = joshi_dd.iloc[:, 4:8]\n",
    "  fset3 = pd.concat([base_df, fset3], axis=1)\n",
    "  fset4 = pd.concat([base_df, joshi_df], axis=1)\n",
    "  \n",
    "  return fset1, fset2, fset3, fset4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVJ5BzLv1Geq"
   },
   "outputs": [],
   "source": [
    "# Will always need this\n",
    "wembedding_similarity_features = joshi_features_new(orginal_data, word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "aHKW-tWY82x_"
   },
   "source": [
    "#### Gonzalez Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "60Wvk-OF9A3x"
   },
   "outputs": [],
   "source": [
    "# Upload LIWC features pkl\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Xa227y5U9RAN"
   },
   "outputs": [],
   "source": [
    "gonz_features = get_gonz_features(original_data, labels, liwc, n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "HTBA9rKlfTNS"
   },
   "outputs": [],
   "source": [
    "gonz, gonz_s, gonz_w, gonz_sw = getFsets(gonz_features, wembedding_similarity_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FV6HkbCU9iMW"
   },
   "source": [
    "#### Liebrechet Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xC_ptOMQ9nqS"
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXK3L2yafTNc"
   },
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "LjcJIGQJfTNf"
   },
   "source": [
    "#### SVMperf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "tYfRoUU3fTNi"
   },
   "outputs": [],
   "source": [
    "def shuffle(df):\n",
    "    '''\n",
    "        Shuffles pandas Dataframe\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Dataframe to be shuffled\n",
    "    \n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        df_copy : Pandas DataFrame\n",
    "            Shuffled Dataframe copy is returned    \n",
    "    '''\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.sample(frac=1).reset_index(drop=True)\n",
    "    return df_copy\n",
    "\n",
    "# For SVMperf\n",
    "\n",
    "def store(file_name,data,labels):\n",
    "#     <line> .=. <target> <feature>:<value> <feature>:<value> ... <feature>:<value> # <info>\n",
    "#     <target> .=. {+1,-1}\n",
    "#     <feature> .=. <integer>\n",
    "#     <value> .=. <float>\n",
    "#     <info> .=. <string>\n",
    "    labels = 2*labels - 1\n",
    "    with open(file_name+'.dat','w') as f :\n",
    "        final_string = \"\"\n",
    "        for idx,row in data.iterrows():\n",
    "            string = ''\n",
    "            string += '+1 ' if labels[idx] ==1 else '-1 '\n",
    "            column_indices = data.columns\n",
    "            for index in column_indices:\n",
    "                if row[index] == 0:\n",
    "                    continue\n",
    "                string += str((index+1))+':'+str(row[index])+' '\n",
    "            string += '\\n'\n",
    "            final_string += string\n",
    "        f.write(final_string[:-1])\n",
    "        print('Finished Storing')\n",
    "        return final_string\n",
    "\n",
    "def customKfold(data, labels, n_folds=5, random_seed=99):\n",
    "    # Note: Will not shuffle data - Shuffle before and pass\n",
    "    '''\n",
    "        Stores train and test .dat files for each fold\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Feature vectors\n",
    "        labels: list\n",
    "            True Labels\n",
    "        n_folds:\n",
    "            Number of folds required\n",
    "        random_seed: \n",
    "            Random seed to generate the split\n",
    "    '''\n",
    "    from sklearn.model_selection import KFold\n",
    "    kfold = KFold(n_folds, True, random_seed)\n",
    "    iteration = 1\n",
    "    for train, test in kfold.split(data):\n",
    "        print(iteration)\n",
    "        X_train, X_test, y_train, y_test = np.array(data.iloc[train]), np.array(data.iloc[test]), labels[train], labels[test]\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        store('train_'+str(iteration), X_train, y_train)\n",
    "        store('test_'+str(iteration), X_test, y_test)\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "LeECiQ8zt7Fq"
   },
   "outputs": [],
   "source": [
    "customKfold(joshi_df_new, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "g-XzzbPouf2L"
   },
   "outputs": [],
   "source": [
    "# Set this for number of Folds to run\n",
    "N_FOLDS = 5\n",
    "output_list = []\n",
    "for i in range(0, N_FOLDS):\n",
    "  bashCommand = \"./svm_perf_learn -c 20.0 -l 1 -w 3 \"+\"train_\"+str(i)+\".dat\"+\"model_\"+str(i)+\".dat\"\n",
    "  process = subprocess.Popen(shlex.split(bashCommand), stdout=subprocess.PIPE)\n",
    "  output, error = process.communicate()\n",
    "  output_list.append([output, error])\n",
    "\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "dvtCljObuDFC"
   },
   "outputs": [],
   "source": [
    "!./svm_perf_learn -c 20.0 -l 1 -w 3 train_1.dat model_1.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "KnGI5-tAuFX3"
   },
   "outputs": [],
   "source": [
    "!./svm_perf_classify test_1.dat model_1.dat predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIyNs42FfTNz"
   },
   "source": [
    "#### SVMrbf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "imI4fE0UfTOH"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = svm.SVC(gamma='scale', class_weight='balanced', C=20.0, cache_size=1000)\n",
    "\n",
    "def classify(data, labels):\n",
    "    # 10 is pseudo random number\n",
    "    kfold = KFold(5, True, 100)\n",
    "    scores = []\n",
    "    i = 0\n",
    "    for train, test in kfold.split(data):\n",
    "        print(i)\n",
    "        X_train, X_test, y_train, y_test = data.iloc[train], data.iloc[test], labels[train], labels[test]\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        metric = precision_recall_fscore_support(y_test, y_pred)\n",
    "        scores.append(metric)\n",
    "        print(\"Done Interation: %d\"%(i))\n",
    "        i += 1\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cell_style": "center",
    "colab": {},
    "colab_type": "code",
    "id": "7kDIKVYkfTON"
   },
   "outputs": [],
   "source": [
    "score = classify(gonz_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjItPteMfTOU"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "x = 1027\n",
    "y_pred = model.predict(gonz_features)\n",
    "confusion = confusion_matrix(labels, y_pred)\n",
    "pprint.pprint(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-T4N74Bd-PTa"
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJjU8yjofTOc"
   },
   "outputs": [],
   "source": [
    "plt.title('precision')\n",
    "plt.plot([x for x in range(len(scores))],[score[0][1] for score in scores])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oM6CbfGtfTOi"
   },
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqoFrrsBfTOn"
   },
   "outputs": [],
   "source": [
    "score = scores\n",
    "plt.title('recall')\n",
    "plt.plot([x for x in range(len(scores))],[score[1][1] for score in scores])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BtzGRAzAfTOt"
   },
   "outputs": [],
   "source": [
    "score = scores\n",
    "plt.title('F-score')\n",
    "plt.plot([x for x in range(len(scores))],[score[2][1] for score in scores])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l_nHCnt7fTOx"
   },
   "outputs": [],
   "source": [
    "score = scores\n",
    "f_scores = [score[2][1] for score in scores]\n",
    "print(np.array(f_scores).mean())\n",
    "# confidence intervals\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower = max(0.0, np.percentile(f_scores, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, np.percentile(f_scores, p))\n",
    "print('%.1f confidence interval: %.1f and %.1f' % (alpha*100, lower*100, upper*100))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Joshi_Paper.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
