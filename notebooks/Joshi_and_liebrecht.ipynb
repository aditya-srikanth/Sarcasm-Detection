{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk import TreebankWordTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from pprint import pprint\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset and preprocess\n",
    "data = pd.read_csv('..\\\\sarcasm_dataset_clean.csv')          # import the data\n",
    "labels = np.array(data['label'].tolist())                    # extracts the labels as a list \n",
    "text = data['text'].tolist()                                 # extracts the text to be processed as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tok = TreebankWordTokenizer()\n",
    "# tokenizer=tok.tokenize, use this if the results are poor\n",
    "minfreq = 3                                              # Used to prune those terms (uni-|bi-|tri- grams whose frequency is less than this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you remember me telling you we are practicing non-verbal spells, Potter?\" \"Yes,\" said Harry stiffly. \"Yes, sir.\" \"There's no need to call me \"sir\" Professor.\" The words had escaped him before he knew what he was saying.  '\n"
     ]
    }
   ],
   "source": [
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(text,labels,n_gram,tokenizer=None,min_document_frequency=2,minfreq=3):\n",
    "    '''\n",
    "        Tokenizes the dataset and return the term-frequency matrix of the frequent n-grams \n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        text : list\n",
    "            List of strings containing the text to be processed.\n",
    "        n_gram: int\n",
    "            The size of the n-gram to be returned\n",
    "        tokenizer:\n",
    "            String tokenizer to be used\n",
    "        min_document_frequency: \n",
    "            The minimum document frequency to be used to prune n-grams\n",
    "        minfreq: \n",
    "            The minimum frequency to be used to prune n-grams\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dataframe : Pandas DataFrame\n",
    "            The text converted as a term-frequency matrix\n",
    "        n_grams : list\n",
    "            The list of n-grams that were generated\n",
    "        weights: tuple \n",
    "            p_value , chi2 value \n",
    "    '''\n",
    "    vectorizer = CountVectorizer(tokenizer=tokenizer,lowercase=False,\\\n",
    "                                 ngram_range=(n_gram,n_gram),\\\n",
    "                                 min_df=min_document_frequency,\\\n",
    "                                 stop_words='english',\\\n",
    "                                 token_pattern='\\\\w+')\n",
    "    \n",
    "    processed_data = (vectorizer.fit_transform(text))                      \n",
    "    processed_data = processed_data.toarray()                              \n",
    "    \n",
    "    n_grams = vectorizer.get_feature_names()                               \n",
    "    \n",
    "    counts = np.sum(processed_data,axis=0)                                 \n",
    "    indices_to_keep = (np.argwhere(counts > minfreq)).flatten()            \n",
    "    processed_data = processed_data[:,indices_to_keep] \n",
    "#     print(type(indices_to_keep))\n",
    "    n_grams = [n_grams[i]  for i in indices_to_keep]\n",
    "    dataframe = pd.DataFrame(processed_data)\n",
    "    \n",
    "    weights = chi2(processed_data,labels)\n",
    "    \n",
    "    return dataframe,n_grams,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(file_name,data,labels):\n",
    "#     <line> .=. <target> <feature>:<value> <feature>:<value> ... <feature>:<value> # <info>\n",
    "#     <target> .=. {+1,-1}\n",
    "#     <feature> .=. <integer>\n",
    "#     <value> .=. <float>\n",
    "#     <info> .=. <string>\n",
    "    labels = 2*labels - 1\n",
    "    with open(file_name+'.dat','w') as f :\n",
    "        final_string = \"\"\n",
    "        for idx,row in data.iterrows():\n",
    "            string = ''\n",
    "            string += '+1 ' if labels[idx] ==1 else '-1 '\n",
    "            column_indices = data.columns\n",
    "            for index in column_indices:\n",
    "                if row[index] == 0:\n",
    "                    continue\n",
    "                string += str((index+1))+':'+str(row[index])+' '\n",
    "            string += '\\n'\n",
    "            final_string += string\n",
    "        f.write(final_string[:-1])\n",
    "        print('Finished Storing')\n",
    "        return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3756, 3469)\n"
     ]
    }
   ],
   "source": [
    "unigrams_matrix,unigrams,(unigram_chi,unigram_p) = process_data(text,labels,1)\n",
    "unigram_features = unigrams_matrix*unigram_chi\n",
    "print(unigrams_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3756, 594)\n"
     ]
    }
   ],
   "source": [
    "bigrams_matrix,bigrams,(bigram_chi,bigram_p) = process_data(text,labels,2)\n",
    "bigram_features = bigrams_matrix*bigram_chi\n",
    "print(bigrams_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3756, 63)\n"
     ]
    }
   ],
   "source": [
    "trigrams_matrix,trigrams,(trigram_chi,trigram_p) = process_data(text,labels,3)\n",
    "trigram_features = trigrams_matrix*trigram_chi\n",
    "print(trigrams_matrix.shape)\n",
    "# print(trigrams)\n",
    "# print(trigrams_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Storing\n"
     ]
    }
   ],
   "source": [
    "final_string = store('train_data',trigrams_matrix,labels)\n",
    "from sklearn.model_selection import KFold\n",
    "# print(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(df):\n",
    "    '''\n",
    "        Shuffles pandas Dataframe\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        df : pd.DataFrame\n",
    "            Dataframe to be shuffled\n",
    "    \n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        df_copy : Pandas DataFrame\n",
    "            Shuffled Dataframe copy is returned    \n",
    "    '''\n",
    "    df_copy = df.copy()\n",
    "    df_copy = df_copy.sample(frac=1).reset_index(drop=True)\n",
    "    return df_copy\n",
    "\n",
    "def customKfold(data, labels, n_folds=5, random_seed=99):\n",
    "    # Note: Will not shuffle data - Shuffle before and pass\n",
    "    '''\n",
    "        Stores train and test .dat files for each fold\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Feature vectors\n",
    "        labels: list\n",
    "            True Labels\n",
    "        n_folds:\n",
    "            Number of folds required\n",
    "        random_seed: \n",
    "            Random seed to generate the split\n",
    "    '''\n",
    "    from sklearn.model_selection import KFold\n",
    "    kfold = KFold(n_folds, True, random_seed)\n",
    "    iteration = 1\n",
    "    for train, test in kfold.split(data):\n",
    "        print(iteration)\n",
    "        X_train, X_test, y_train, y_test = np.array(data.iloc[train]), np.array(data.iloc[test]), labels[train], labels[test]\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        store('data/train_'+str(iteration), X_train, y_train)\n",
    "        store('data/test_'+str(iteration), X_test, y_test)\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieb_f = pd.concat([unigram_features, bigram_features, trigram_features], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Finished Storing\n",
      "Finished Storing\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "features = shuffle(lieb_f)\n",
    "start_time = time.time()\n",
    "customKfold(features, labels)\n",
    "end_time = time.time()-start_time\n",
    "print(end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
